#!/usr/bin/env python3
"""
GitHub Actions Node Selector
è‡ªåŠ¨æµ‹è¯•èŠ‚ç‚¹å»¶è¿Ÿã€é€Ÿåº¦ï¼Œå¹¶ç”Ÿæˆä¼˜é€‰èŠ‚ç‚¹åˆ—è¡¨
æ”¯æŒåœ¨çº¿è®¢é˜…å’Œæ‰‹åŠ¨è¿è¡Œ
"""

import os
import json
import time
import requests
import base64
import re
import sys
import argparse
import random
from datetime import datetime
from urllib.parse import urlparse
import concurrent.futures
import threading

class NodeSelector:
    def __init__(self, args):
        self.nodes_file = args.nodes_file
        self.output_file = args.output_file
        self.results_file = args.results_file
        
        # å‘½ä»¤è¡Œå‚æ•°
        self.args = args
        
        # ä»ç¯å¢ƒå˜é‡æˆ–å‘½ä»¤è¡Œå‚æ•°è·å–åœ¨çº¿è®¢é˜…åœ°å€
        self.subscription_urls = self.get_subscription_urls()
        
        # æµ‹è¯•é…ç½®
        self.timeout = args.timeout
        self.latency_threshold = args.latency_threshold
        self.max_workers = args.workers
        self.test_count = args.test_count
        
        # æµ‹è¯•URLåˆ—è¡¨
        self.test_urls = [
            {
                'url': 'https://www.gstatic.com/generate_204',
                'name': 'Google Static',
                'expected_status': 204,
                'weight': 1.0
            },
            {
                'url': 'https://httpbin.org/get',
                'name': 'HttpBin', 
                'expected_status': 200,
                'weight': 0.9
            },
            {
                'url': 'https://www.cloudflare.com/cdn-cgi/trace',
                'name': 'Cloudflare',
                'expected_status': 200,
                'weight': 0.8
            },
            {
                'url': 'https://api.github.com',
                'name': 'GitHub',
                'expected_status': 200,
                'weight': 0.7
            }
        ]
        
        self.results = []
        self.lock = threading.Lock()
        
    def get_subscription_urls(self):
        """ä»ç¯å¢ƒå˜é‡æˆ–å‘½ä»¤è¡Œå‚æ•°è·å–åœ¨çº¿è®¢é˜…åœ°å€"""
        # ä¼˜å…ˆä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°
        if self.args.subscription:
            urls = [url.strip() for url in self.args.subscription.split('&') if url.strip()]
            print(f"ğŸ“¡ ä»å‘½ä»¤è¡Œå‚æ•°æ‰¾åˆ° {len(urls)} ä¸ªåœ¨çº¿è®¢é˜…åœ°å€")
            return urls
        
        # å…¶æ¬¡ä½¿ç”¨ç¯å¢ƒå˜é‡
        subscription_env = os.getenv('ONLINE_SUBSCRIPTION', '').strip()
        if subscription_env:
            urls = [url.strip() for url in subscription_env.split('&') if url.strip()]
            print(f"ğŸ“¡ ä»ç¯å¢ƒå˜é‡æ‰¾åˆ° {len(urls)} ä¸ªåœ¨çº¿è®¢é˜…åœ°å€")
            return urls
        
        return []
    
    def fetch_online_subscription(self, url):
        """è·å–åœ¨çº¿è®¢é˜…å†…å®¹"""
        try:
            print(f"ğŸ”— è·å–è®¢é˜…: {url}")
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            
            response = requests.get(url, timeout=15, headers=headers)
            response.raise_for_status()
            
            # å°è¯•Base64è§£ç 
            try:
                content = base64.b64decode(response.text).decode('utf-8')
                print(f"âœ… è®¢é˜…è§£ç æˆåŠŸï¼Œé•¿åº¦: {len(content)} å­—ç¬¦")
                return content
            except:
                # å¦‚æœä¸æ˜¯Base64ï¼Œç›´æ¥ä½¿ç”¨åŸå†…å®¹
                print(f"âœ… è®¢é˜…è·å–æˆåŠŸï¼Œé•¿åº¦: {len(response.text)} å­—ç¬¦")
                return response.text
                
        except Exception as e:
            print(f"âŒ è·å–è®¢é˜…å¤±è´¥ [{url}]: {e}")
            return None
    
    def parse_subscription_content(self, content):
        """è§£æè®¢é˜…å†…å®¹"""
        nodes = []
        lines = content.split('\n')
        
        for line in lines:
            line = line.strip()
            if not line or line.startswith('#'):
                continue
            
            # æ”¯æŒå„ç§ä»£ç†åè®®
            if any(proto in line for proto in ['ss://', 'ssr://', 'vmess://', 'trojan://', 'vless://']):
                node = self.parse_node_line(line)
                if node:
                    nodes.append(node)
                    # æ ‡è®°æ¥è‡ªè®¢é˜…
                    node['source'] = 'subscription'
        
        return nodes
    
    def load_all_nodes(self):
        """åŠ è½½æ‰€æœ‰èŠ‚ç‚¹ï¼ˆæœ¬åœ°æ–‡ä»¶ + åœ¨çº¿è®¢é˜…ï¼‰"""
        all_nodes = []
        
        # 1. åŠ è½½æœ¬åœ°èŠ‚ç‚¹æ–‡ä»¶
        local_nodes = self.parse_nodes_file()
        for node in local_nodes:
            node['source'] = 'local'
        all_nodes.extend(local_nodes)
        print(f"ğŸ“ æœ¬åœ°èŠ‚ç‚¹: {len(local_nodes)} ä¸ª")
        
        # 2. åŠ è½½åœ¨çº¿è®¢é˜…èŠ‚ç‚¹
        subscription_nodes = []
        for sub_url in self.subscription_urls:
            try:
                content = self.fetch_online_subscription(sub_url)
                if content:
                    nodes = self.parse_subscription_content(content)
                    subscription_nodes.extend(nodes)
                    print(f"ğŸ“¥ ä»è®¢é˜…è·å–èŠ‚ç‚¹: {len(nodes)} ä¸ª")
                    
                    # çŸ­æš‚å»¶è¿Ÿé¿å…è¯·æ±‚è¿‡å¿«
                    time.sleep(1)
                    
            except Exception as e:
                print(f"âŒ å¤„ç†è®¢é˜…å¤±è´¥ [{sub_url}]: {e}")
        
        all_nodes.extend(subscription_nodes)
        
        # å»é‡ï¼ˆåŸºäºåŸå§‹é…ç½®ï¼‰
        unique_nodes = []
        seen = set()
        
        for node in all_nodes:
            node_id = node['original']
            if node_id not in seen:
                seen.add(node_id)
                unique_nodes.append(node)
        
        # å¦‚æœæŒ‡å®šäº†æµ‹è¯•æ•°é‡ï¼Œè¿›è¡ŒæŠ½æ ·
        if self.test_count > 0 and len(unique_nodes) > self.test_count:
            print(f"ğŸ”¢ æŠ½æ ·æµ‹è¯•: ä» {len(unique_nodes)} ä¸ªèŠ‚ç‚¹ä¸­éšæœºé€‰æ‹© {self.test_count} ä¸ª")
            unique_nodes = random.sample(unique_nodes, self.test_count)
        
        print(f"ğŸ“Š æ€»èŠ‚ç‚¹æ•°: {len(all_nodes)} â†’ å»é‡å: {len(unique_nodes)} ä¸ª")
        return unique_nodes
    
    def parse_nodes_file(self):
        """è§£æèŠ‚ç‚¹æ–‡ä»¶"""
        nodes = []
        if not os.path.exists(self.nodes_file):
            print(f"âš ï¸ èŠ‚ç‚¹æ–‡ä»¶ {self.nodes_file} ä¸å­˜åœ¨")
            return nodes
            
        try:
            with open(self.nodes_file, 'r', encoding='utf-8') as f:
                for line_num, line in enumerate(f, 1):
                    line = line.strip()
                    if not line or line.startswith('#'):
                        continue
                    
                    node = self.parse_node_line(line)
                    if node:
                        nodes.append(node)
                    else:
                        print(f"âš ï¸ ç¬¬{line_num}è¡Œæ— æ³•è§£æ: {line[:50]}...")
                        
            return nodes
            
        except Exception as e:
            print(f"âŒ è¯»å–èŠ‚ç‚¹æ–‡ä»¶å¤±è´¥: {e}")
            return []
    
    def parse_node_line(self, line):
        """è§£æå•è¡ŒèŠ‚ç‚¹é…ç½®"""
        line = line.strip()
        
        patterns = [
            # SSRæ ¼å¼
            {'regex': r'^ssr://([A-Za-z0-9+/=]+)', 'type': 'ssr'},
            # VMessæ ¼å¼  
            {'regex': r'^vmess://([A-Za-z0-9+/=]+)', 'type': 'vmess'},
            # Trojanæ ¼å¼
            {'regex': r'^trojan://([^@]+)@([^:]+):(\d+)', 'type': 'trojan'},
            # VLESSæ ¼å¼
            {'regex': r'^vless://([^@]+)@([^:]+):(\d+)', 'type': 'vless'},
            # SSæ ¼å¼
            {'regex': r'^ss://([A-Za-z0-9+/=]+)', 'type': 'ss'},
            # HTTPä»£ç†
            {'regex': r'^http://([^:]+):(\d+)', 'type': 'http'},
            # SOCKS5ä»£ç†
            {'regex': r'^socks5://([^:]+):(\d+)', 'type': 'socks5'},
            # ä¸»æœºç«¯å£æ ¼å¼
            {'regex': r'^([^:]+):(\d+)$', 'type': 'host-port'}
        ]
        
        for pattern in patterns:
            match = re.match(pattern['regex'], line)
            if match:
                return {
                    'original': line,
                    'type': pattern['type'],
                    'parts': match.groups()
                }
        
        return None
    
    def extract_host_from_node(self, node):
        """ä»èŠ‚ç‚¹é…ç½®ä¸­æå–ä¸»æœºåœ°å€"""
        try:
            if node['type'] in ['ssr', 'vmess', 'ss']:
                # Base64è§£ç 
                decoded = base64.b64decode(node['parts'][0] + '==').decode('utf-8', errors='ignore')
                
                # å°è¯•å¤šç§æ–¹å¼æå–ä¸»æœºå
                host_patterns = [
                    r'"add":"([^"]+)"',      # VMessæ ¼å¼
                    r'server=([^&]+)',       # å‚æ•°æ ¼å¼
                    r'@([^:]+):',            # ç”¨æˆ·ä¿¡æ¯æ ¼å¼
                    r'([a-zA-Z0-9.-]+\.[a-zA-Z]{2,})'  # é€šç”¨åŸŸåæ ¼å¼
                ]
                
                for pattern in host_patterns:
                    match = re.search(pattern, decoded)
                    if match:
                        return match.group(1)
                        
            elif node['type'] in ['trojan', 'vless']:
                return node['parts'][1]  # ä¸»æœºå
            elif node['type'] in ['http', 'socks5', 'host-port']:
                return node['parts'][0]  # ä¸»æœºå
                
        except Exception as e:
            print(f"âš ï¸ æå–ä¸»æœºåœ°å€å¤±è´¥: {e}")
            
        return None
    
    def test_latency(self, node):
        """æµ‹è¯•èŠ‚ç‚¹å»¶è¿Ÿ"""
        test_results = []
        fastest_success = None
        
        for test_url in self.test_urls:
            try:
                start_time = time.time()
                
                response = requests.get(
                    test_url['url'],
                    timeout=8,
                    headers={
                        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
                    }
                )
                
                latency = int((time.time() - start_time) * 1000)  # è½¬æ¢ä¸ºæ¯«ç§’
                is_success = response.status_code == test_url['expected_status']
                
                test_result = {
                    'url': test_url['name'],
                    'latency': latency,
                    'status': response.status_code,
                    'success': is_success,
                    'weight': test_url['weight']
                }
                
                test_results.append(test_result)
                
                # è®°å½•æœ€å¿«æˆåŠŸæµ‹è¯•
                if is_success and latency < self.latency_threshold:
                    if not fastest_success or latency < fastest_success['latency']:
                        fastest_success = test_result
                
                # ä¼˜è´¨èŠ‚ç‚¹æå‰ç»“æŸæµ‹è¯•
                if latency < 100:
                    break
                    
                time.sleep(0.3)  # çŸ­æš‚å»¶è¿Ÿ
                
            except requests.RequestException as e:
                test_results.append({
                    'url': test_url['name'],
                    'latency': -1,
                    'status': 0,
                    'success': False,
                    'error': str(e),
                    'weight': test_url['weight']
                })
        
        return {
            'fastest_success': fastest_success,
            'all_results': test_results,
            'passed': fastest_success is not None
        }
    
    def test_download_speed(self, node, latency):
        """æµ‹è¯•ä¸‹è½½é€Ÿåº¦"""
        print(f"    ğŸš€ å¼€å§‹é€Ÿåº¦æµ‹è¯•ï¼Œå½“å‰å»¶è¿Ÿ: {latency}ms")
        
        # æ ¹æ®å»¶è¿Ÿè°ƒæ•´æµ‹è¯•æ–‡ä»¶å¤§å°
        if latency < 200:
            file_size = 512000  # 500KB
        elif latency < 500:
            file_size = 256000  # 250KB
        else:
            file_size = 102400  # 100KB
        
        speed_test_urls = [
            f'https://httpbin.org/bytes/{file_size}',
            'https://speedtest.ftp.otenet.gr/files/test1Mb.db',
            'https://proof.ovh.net/files/1Mb.dat'
        ]
        
        for test_url in speed_test_urls:
            try:
                start_time = time.time()
                
                response = requests.get(
                    test_url,
                    timeout=15,
                    stream=True,
                    headers={
                        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
                        'Cache-Control': 'no-cache'
                    }
                )
                response.raise_for_status()
                
                # è¯»å–å®Œæ•´å†…å®¹ä»¥ç¡®ä¿æµ‹é‡å‡†ç¡®
                content = b''
                for chunk in response.iter_content(chunk_size=8192):
                    content += chunk
                
                duration = time.time() - start_time
                data_size = len(content)
                
                if data_size > 0 and duration > 0:
                    speed_kbps = (data_size / duration) / 1024  # KB/s
                    speed_mbps = speed_kbps / 1024  # MB/s
                    
                    print(f"    ğŸ“Š é€Ÿåº¦æµ‹è¯•å®Œæˆ: {speed_kbps:.0f} KB/s ({speed_mbps:.2f} MB/s)")
                    return int(speed_kbps)
                    
            except requests.RequestException:
                continue
                
            time.sleep(0.5)
        
        print("    âš ï¸ æ‰€æœ‰æµ‹é€ŸURLå‡å¤±è´¥")
        return 0
    
    def get_geo_info(self):
        """è·å–åœ°ç†ä½ç½®ä¿¡æ¯"""
        try:
            # è·å–å…¬ç½‘IP
            ip_response = requests.get('https://httpbin.org/ip', timeout=8)
            if ip_response.status_code == 200:
                ip_data = ip_response.json()
                public_ip = ip_data.get('origin', '').split(',')[0]
                
                if public_ip:
                    # è·å–åœ°ç†ä½ç½®
                    geo_response = requests.get(f'http://ip-api.com/json/{public_ip}', timeout=5)
                    if geo_response.status_code == 200:
                        geo_data = geo_response.json()
                        if geo_data.get('status') == 'success':
                            return {
                                'country': geo_data.get('country', 'Unknown'),
                                'city': geo_data.get('city', 'Unknown'),
                                'isp': geo_data.get('isp', 'Unknown'),
                                'lat': geo_data.get('lat'),
                                'lon': geo_data.get('lon'),
                                'ip': public_ip
                            }
        except requests.RequestException:
            pass
        
        return {
            'country': 'Unknown',
            'city': 'Unknown', 
            'isp': 'Unknown',
            'lat': None,
            'lon': None,
            'ip': 'Unknown'
        }
    
    def calculate_score(self, latency, speed, success):
        """è®¡ç®—ç»¼åˆè¯„åˆ†"""
        if latency <= 0:
            return 0
        
        # å»¶è¿Ÿè¯„åˆ†
        if latency < 50:
            latency_score = 100
        elif latency < 100:
            latency_score = 95
        elif latency < 200:
            latency_score = 85
        elif latency < 300:
            latency_score = 75
        elif latency < 500:
            latency_score = 60
        elif latency < 1000:
            latency_score = 40
        else:
            latency_score = 20
        
        # é€Ÿåº¦è¯„åˆ†
        if speed == 0:
            speed_score = 0
        elif speed > 10000:
            speed_score = 100
        elif speed > 5000:
            speed_score = 90
        elif speed > 2000:
            speed_score = 80
        elif speed > 1000:
            speed_score = 70
        elif speed > 500:
            speed_score = 60
        elif speed > 100:
            speed_score = 40
        else:
            speed_score = 20
        
        # æˆåŠŸç‡è¯„åˆ†
        success_score = 100 if success else 0
        
        # åŠ æƒè¯„åˆ†
        total_score = (latency_score * 0.5 + speed_score * 0.3 + success_score * 0.2)
        return round(total_score, 1)
    
    def test_single_node(self, node, index, total_count):
        """æµ‹è¯•å•ä¸ªèŠ‚ç‚¹"""
        node_id = f"{index+1}/{total_count}"
        source_info = f"[{node.get('source', 'unknown')}]"
        print(f"\nğŸ” æµ‹è¯•èŠ‚ç‚¹ {node_id} {source_info}: {node['type']}èŠ‚ç‚¹")
        print(f"    ğŸ“ é…ç½®: {node['original'][:80]}...")
        
        try:
            # ç¬¬ä¸€æ­¥ï¼šå»¶è¿Ÿæµ‹è¯•
            latency_test = self.test_latency(node)
            
            if not latency_test['passed']:
                print(f"    âŒ æœªé€šè¿‡å»¶è¿Ÿæµ‹è¯•ï¼Œè·³è¿‡æµ‹é€Ÿ")
                result = {
                    'node': node['original'],
                    'type': node['type'],
                    'latency': 'Timeout',
                    'speed': 'Not Tested',
                    'country': 'Unknown',
                    'city': 'Unknown',
                    'isp': 'Unknown',
                    'ip': 'Unknown',
                    'score': 0,
                    'success': False,
                    'test_url': 'None',
                    'timestamp': datetime.now().isoformat(),
                    'skipped_speed_test': True,
                    'source': node.get('source', 'unknown')
                }
                return result
            
            latency = latency_test['fastest_success']['latency']
            print(f"    âœ… å»¶è¿Ÿæµ‹è¯•é€šè¿‡: {latency}ms")
            
            # ç¬¬äºŒæ­¥ï¼šè·å–åœ°ç†ä½ç½®
            geo_info = self.get_geo_info()
            print(f"    ğŸŒ åœ°ç†ä½ç½®: {geo_info['country']}/{geo_info['city']} ({geo_info['isp']})")
            
            # ç¬¬ä¸‰æ­¥ï¼šé€Ÿåº¦æµ‹è¯•
            speed = 0
            if latency < self.latency_threshold:
                speed = self.test_download_speed(node, latency)
            else:
                print(f"    âš ï¸ å»¶è¿Ÿè¿‡é«˜ ({latency}ms)ï¼Œè·³è¿‡æµ‹é€Ÿ")
            
            # ç¬¬å››æ­¥ï¼šè®¡ç®—è¯„åˆ†
            score = self.calculate_score(latency, speed, latency_test['fastest_success']['success'])
            
            result = {
                'node': node['original'],
                'type': node['type'],
                'latency': latency,
                'speed': f"{speed} KB/s" if speed > 0 else "Failed",
                'country': geo_info['country'],
                'city': geo_info['city'],
                'isp': geo_info['isp'],
                'ip': geo_info['ip'],
                'score': score,
                'success': latency_test['fastest_success']['success'],
                'test_url': latency_test['fastest_success']['url'],
                'timestamp': datetime.now().isoformat(),
                'source': node.get('source', 'unknown')
            }
            
            print(f"    ğŸ“Š ç»¼åˆè¯„åˆ†: {score}")
            return result
            
        except Exception as e:
            print(f"    âŒ æµ‹è¯•å¤±è´¥: {e}")
            return None
    
    def run_tests(self):
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
        print("ğŸš€ å¼€å§‹èŠ‚ç‚¹æµ‹è¯•...")
        print(f"ğŸ“¡ æµ‹è¯•URL: {[u['name'] for u in self.test_urls]}")
        print(f"â±ï¸ å»¶è¿Ÿé˜ˆå€¼: {self.latency_threshold}ms")
        print(f"ğŸ”¢ æœ€å¤§å¹¶å‘æ•°: {self.max_workers}")
        print(f"â° è¶…æ—¶æ—¶é—´: {self.timeout}ç§’")
        
        if self.test_count > 0:
            print(f"ğŸ¯ æµ‹è¯•æ•°é‡: {self.test_count} ä¸ªèŠ‚ç‚¹")
        
        # æ˜¾ç¤ºè®¢é˜…ä¿¡æ¯
        if self.subscription_urls:
            print(f"ğŸŒ åœ¨çº¿è®¢é˜…: {len(self.subscription_urls)} ä¸ª")
            for i, url in enumerate(self.subscription_urls, 1):
                print(f"    {i}. {url}")
        print()
        
        # åŠ è½½æ‰€æœ‰èŠ‚ç‚¹
        nodes = self.load_all_nodes()
        if not nodes:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°å¯æµ‹è¯•çš„èŠ‚ç‚¹")
            return
        
        print(f"ğŸ“Š æ€»å…± {len(nodes)} ä¸ªèŠ‚ç‚¹éœ€è¦æµ‹è¯•\n")
        
        passed_count = 0
        speed_tested_count = 0
        
        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘æµ‹è¯•
        with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            future_to_node = {
                executor.submit(self.test_single_node, node, i, len(nodes)): (i, node)
                for i, node in enumerate(nodes)
            }
            
            for future in concurrent.futures.as_completed(future_to_node):
                i, node = future_to_node[future]
                try:
                    result = future.result()
                    if result:
                        with self.lock:
                            self.results.append(result)
                            
                            if result['success']:
                                passed_count += 1
                                if result['speed'] not in ['Not Tested', 'Failed']:
                                    speed_tested_count += 1
                
                except Exception as e:
                    print(f"âŒ èŠ‚ç‚¹æµ‹è¯•å¼‚å¸¸: {e}")
        
        # æŒ‰è¯„åˆ†æ’åº
        self.results.sort(key=lambda x: x['score'], reverse=True)
        
        # ä¿å­˜ç»“æœ
        self.save_test_results(passed_count, speed_tested_count, len(nodes))
    
    def save_test_results(self, passed_count, speed_tested_count, total_count):
        """ä¿å­˜æµ‹è¯•ç»“æœ"""
        output_data = {
            'timestamp': datetime.now().isoformat(),
            'total_tested': total_count,
            'passed_latency_test': passed_count,
            'speed_tested': speed_tested_count,
            'preferred_nodes': [r for r in self.results if r['score'] > 0][:20],
            'all_results': self.results,
            'subscription_urls': self.subscription_urls,
            'test_config': {
                'urls': self.test_urls,
                'timeout': self.timeout,
                'latency_threshold': self.latency_threshold,
                'max_workers': self.max_workers,
                'test_count': self.test_count
            }
        }
        
        with open(self.results_file, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, indent=2, ensure_ascii=False)
        
        print('\nğŸ‰ æµ‹è¯•å®Œæˆ!')
        print(f"ğŸ“Š æ€»æµ‹è¯•èŠ‚ç‚¹: {total_count}")
        print(f"âœ… é€šè¿‡å»¶è¿Ÿæµ‹è¯•: {passed_count}")
        print(f"ğŸš€ å®Œæˆé€Ÿåº¦æµ‹è¯•: {speed_tested_count}")
        print(f"ğŸ† æœ€ä½³èŠ‚ç‚¹è¯„åˆ†: {self.results[0]['score'] if self.results else 'N/A'}")
        
        # æ˜¾ç¤ºæ¥æºç»Ÿè®¡
        source_stats = {}
        for result in self.results:
            source = result.get('source', 'unknown')
            source_stats[source] = source_stats.get(source, 0) + 1
        
        print(f"ğŸ“¦ èŠ‚ç‚¹æ¥æºç»Ÿè®¡:")
        for source, count in source_stats.items():
            print(f"    {source}: {count} ä¸ª")
    
    def generate_preferred_node_file(self):
        """ç”Ÿæˆä¼˜é€‰èŠ‚ç‚¹æ–‡ä»¶"""
        try:
            with open(self.results_file, 'r', encoding='utf-8') as f:
                test_data = json.load(f)
            
            output = f"""# ğŸš€ Preferred Nodes - ä¼˜é€‰èŠ‚ç‚¹
# Generated: {datetime.fromisoformat(test_data['timestamp']).strftime('%Y-%m-%d %H:%M:%S')}
# Total nodes tested: {test_data['total_tested']}
# Passed latency test: {test_data['passed_latency_test']}
# Speed tested: {test_data['speed_tested']}
# Success rate: {(test_data['passed_latency_test'] / test_data['total_tested'] * 100):.1f}%
# Workers: {test_data['test_config']['max_workers']}
# Timeout: {test_data['test_config']['timeout']}s

"""
            # æ˜¾ç¤ºè®¢é˜…ä¿¡æ¯
            if test_data.get('subscription_urls'):
                output += f"# ğŸŒ Online Subscriptions: {len(test_data['subscription_urls'])}\n"
                for url in test_data['subscription_urls']:
                    output += f"#   {url}\n"
                output += "\n"

            output += """# ğŸ† Top Recommended Nodes (æ¨èèŠ‚ç‚¹)
# Format: è¯„åˆ† | å»¶è¿Ÿ | é€Ÿåº¦ | ä½ç½® | è¿è¥å•† | æ¥æº
# Score | Latency | Speed | Location | ISP | Source

"""
            
            # åªæ˜¾ç¤ºæœ‰é€Ÿåº¦æµ‹è¯•ç»“æœçš„èŠ‚ç‚¹
            valid_nodes = [
                node for node in test_data['preferred_nodes'] 
                if node['speed'] not in ['Not Tested', 'Failed'] and node['score'] > 0
            ]
            
            for i, node in enumerate(valid_nodes):
                status = 'âœ…' if node['success'] else 'âš ï¸'
                speed_value = int(node['speed'].split()[0]) if 'KB/s' in node['speed'] else 0
                speed_mbps = speed_value / 1024
                source = node.get('source', 'unknown')
                
                output += f"""# {status} {i+1}. è¯„åˆ†:{node['score']} | å»¶è¿Ÿ:{node['latency']}ms | é€Ÿåº¦:{speed_mbps:.1f} MB/s | {node['country']} | {node['isp']} | {source}
{node['node']}

"""
            
            if not valid_nodes:
                output += "# âŒ æ²¡æœ‰æ‰¾åˆ°åˆæ ¼çš„èŠ‚ç‚¹ï¼Œè¯·æ£€æŸ¥èŠ‚ç‚¹é…ç½®æˆ–ç½‘ç»œè¿æ¥\n\n"
            
            output += f"# ğŸ“Š All Tested Nodes (æ‰€æœ‰æµ‹è¯•èŠ‚ç‚¹)\n"
            output += f"# Total: {len(test_data['all_results']} nodes\n\n"
            
            for i, node in enumerate(test_data['all_results']):
                status = 'âœ…' if node['success'] else 'âŒ'
                speed_info = node['speed'] if node['speed'] != 'Not Tested' else 'æœªæµ‹é€Ÿ'
                source = node.get('source', 'unknown')
                output += f"# {status} {i+1}. è¯„åˆ†:{node['score']} å»¶è¿Ÿ:{node['latency']}ms é€Ÿåº¦:{speed_info} {node['country']} [{source}]\n"
                output += f"{node['node']}\n"
                
                if (i + 1) % 10 == 0:
                    output += '\n'
            
            # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
            valid_latencies = [
                node['latency'] for node in test_data['all_results'] 
                if node['latency'] != 'Timeout' and isinstance(node['latency'], (int, float))
            ]
            
            output += f"\n# ğŸ“ˆ Statistics (ç»Ÿè®¡ä¿¡æ¯)\n"
            output += f"# Successful nodes: {test_data['passed_latency_test']}\n"
            output += f"# Speed tested nodes: {test_data['speed_tested']}\n"
            
            avg_score = sum(node['score'] for node in test_data['all_results']) / len(test_data['all_results'])
            output += f"# Average score: {avg_score:.1f}\n"
            
            if valid_latencies:
                output += f"# Best latency: {min(valid_latencies)}ms\n"
                output += f"# Average latency: {sum(valid_latencies) / len(valid_latencies):.1f}ms\n"
            
            with open(self.output_file, 'w', encoding='utf-8') as f:
                f.write(output)
            
            print(f"âœ… {self.output_file} æ–‡ä»¶å·²ç”Ÿæˆ")
            
        except Exception as e:
            print(f"âŒ ç”Ÿæˆç»“æœæ–‡ä»¶å¤±è´¥: {e}")

def parse_arguments():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(
        description='GitHub Actions Node Selector - èŠ‚ç‚¹ä¼˜é€‰å™¨',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  # åŸºæœ¬ä½¿ç”¨
  python node_selector.py
  
  # ä½¿ç”¨åœ¨çº¿è®¢é˜…
  python node_selector.py --subscription "https://sub1.com&https://sub2.com"
  
  # è°ƒæ•´å¹¶å‘æ•°å’Œæµ‹è¯•å‚æ•°
  python node_selector.py --workers 5 --timeout 20 --test-count 50
  
  # è‡ªå®šä¹‰æ–‡ä»¶è·¯å¾„
  python node_selector.py --nodes-file my_nodes.txt --output my_results.txt
  
  # å¿«é€Ÿæµ‹è¯•å°‘é‡èŠ‚ç‚¹
  python node_selector.py --workers 3 --test-count 10 --timeout 10
        """
    )
    
    # è®¢é˜…ç›¸å…³
    parser.add_argument('--subscription', '-s', 
                       help='åœ¨çº¿è®¢é˜…åœ°å€ï¼Œå¤šä¸ªç”¨&åˆ†éš”')
    
    # æµ‹è¯•å‚æ•°
    parser.add_argument('--workers', '-w', type=int, default=3,
                       help='å¹¶å‘å·¥ä½œçº¿ç¨‹æ•° (é»˜è®¤: 3)')
    parser.add_argument('--timeout', '-t', type=int, default=10,
                       help='è¯·æ±‚è¶…æ—¶æ—¶é—´(ç§’) (é»˜è®¤: 10)')
    parser.add_argument('--latency-threshold', '-l', type=int, default=3000,
                       help='å»¶è¿Ÿé˜ˆå€¼(æ¯«ç§’)ï¼Œè¶…è¿‡æ­¤å€¼ä¸æµ‹é€Ÿ (é»˜è®¤: 3000)')
    parser.add_argument('--test-count', '-n', type=int, default=0,
                       help='æµ‹è¯•èŠ‚ç‚¹æ•°é‡ï¼Œ0è¡¨ç¤ºæµ‹è¯•æ‰€æœ‰ (é»˜è®¤: 0)')
    
    # æ–‡ä»¶è·¯å¾„
    parser.add_argument('--nodes-file', '-i', default='Nodes',
                       help='è¾“å…¥èŠ‚ç‚¹æ–‡ä»¶è·¯å¾„ (é»˜è®¤: Nodes)')
    parser.add_argument('--output-file', '-o', default='Preferred-Node',
                       help='è¾“å‡ºç»“æœæ–‡ä»¶è·¯å¾„ (é»˜è®¤: Preferred-Node)')
    parser.add_argument('--results-file', '-r', default='test-results.json',
                       help='æµ‹è¯•ç»“æœJSONæ–‡ä»¶è·¯å¾„ (é»˜è®¤: test-results.json)')
    
    return parser.parse_args()

def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("GitHub Actions Node Selector")
    print("èŠ‚ç‚¹ä¼˜é€‰å™¨ v2.0 - æ”¯æŒæ‰‹åŠ¨è¿è¡Œå’Œåœ¨çº¿è®¢é˜…")
    print("=" * 60)
    
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    args = parse_arguments()
    
    selector = NodeSelector(args)
    
    # è¿è¡Œæµ‹è¯•
    selector.run_tests()
    
    # ç”Ÿæˆç»“æœæ–‡ä»¶
    selector.generate_preferred_node_file()
    
    print("\nğŸŠ æ‰€æœ‰ä»»åŠ¡å®Œæˆ!")

if __name__ == "__main__":
    main()
